{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x203f5c3ced0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {}\n",
    "label_to_ix = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('instructions.txt','r') as f:\n",
    "\n",
    "\tf = f.readlines()\n",
    "\t\n",
    "\tfor line in f:\n",
    "\n",
    "\t\tline = line.strip().split(',')\n",
    "\t\tlabel = [line[1]]\n",
    "\t\tsentence = list(map(lambda x : x.lower(),line[0].strip().split(' ')))\n",
    "\t\tinstructions.append((sentence,label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['climb', 'down', 'the', 'ladder'], ['ladder']), (['climb', 'down', 'the', 'ladder'], ['ladder']), (['climb', 'down', 'the', 'ladder'], ['ladder']), (['climb', 'down', 'the', 'ladder'], ['ladder']), (['climb', 'down', 'the', 'ladder'], ['ladder']), (['climb', 'up', 'the', 'ladder'], ['ladder']), (['climb', 'up', 'the', 'ladder'], ['ladder']), (['climb', 'up', 'the', 'ladder'], ['ladder']), (['climb', 'up', 'the', 'ladder'], ['ladder']), (['get', 'the', 'key'], ['key']), (['get', 'the', 'key'], ['key']), (['get', 'the', 'sword'], ['sword']), (['get', 'the', 'torch'], ['torch']), (['go', 'between', 'the', 'lasers'], ['laser']), (['go', 'between', 'the', 'lasers'], ['laser']), (['go', 'between', 'the', 'lasers'], ['laser']), (['go', 'between', 'the', 'lasers'], ['laser']), (['go', 'to', 'the', 'bottom', 'of', 'the', 'room'], ['room']), (['go', 'to', 'the', 'bottom', 'of', 'the', 'room'], ['room']), (['go', 'to', 'the', 'bottom', 'of', 'the', 'room'], ['room']), (['go', 'to', 'the', 'bottom', 'of', 'the', 'room'], ['room']), (['go', 'to', 'the', 'bottom', 'of', 'the', 'room'], ['room']), (['go', 'to', 'the', 'bottom', 'room'], ['room']), (['go', 'to', 'the', 'bottom', 'room'], ['room']), (['go', 'to', 'the', 'bottom', 'room'], ['room']), (['go', 'to', 'the', 'center', 'of', 'the', 'room'], ['room']), (['go', 'to', 'the', 'center', 'of', 'the', 'room'], ['room']), (['go', 'to', 'the', 'center', 'of', 'the', 'room'], ['room']), (['go', 'to', 'the', 'center', 'of', 'the', 'room'], ['room']), (['go', 'to', 'the', 'center', 'of', 'the', 'room'], ['room']), (['go', 'to', 'the', 'center', 'of', 'the', 'room'], ['room']), (['go', 'to', 'the', 'center', 'of', 'the', 'room'], ['room']), (['go', 'to', 'the', 'center', 'of', 'the', 'room'], ['room']), (['go', 'to', 'the', 'center', 'of', 'the', 'room'], ['room']), (['go', 'to', 'the', 'center', 'of', 'the', 'room'], ['room']), (['go', 'to', 'the', 'center', 'of', 'the', 'room'], ['room']), (['go', 'to', 'the', 'center', 'of', 'the', 'room'], ['room']), (['go', 'to', 'the', 'left', 'room'], ['room']), (['go', 'to', 'the', 'left', 'room'], ['room']), (['go', 'to', 'the', 'left', 'side', 'of', 'the', 'room'], ['room']), (['go', 'to', 'the', 'left', 'side', 'of', 'the', 'room'], ['room']), (['go', 'to', 'the', 'left', 'side', 'of', 'the', 'room'], ['room']), (['go', 'to', 'the', 'left', 'side', 'of', 'the', 'room'], ['room']), (['go', 'to', 'the', 'right', 'room'], ['room']), (['go', 'to', 'the', 'right', 'room'], ['room']), (['go', 'to', 'the', 'right', 'side', 'of', 'the', 'room'], ['room']), (['go', 'to', 'the', 'right', 'side', 'of', 'the', 'room'], ['room']), (['go', 'to', 'the', 'right', 'side', 'of', 'the', 'room'], ['room']), (['go', 'to', 'the', 'right', 'side', 'of', 'the', 'room'], ['room']), (['go', 'to', 'the', 'top', 'of', 'the', 'room'], ['room']), (['go', 'to', 'the', 'top', 'of', 'the', 'room'], ['room']), (['go', 'to', 'the', 'top', 'room'], ['room']), (['jump', 'to', 'the', 'rope'], ['rope']), (['jump', 'to', 'the', 'rope'], ['rope']), (['jump', 'to', 'the', 'rope'], ['rope']), (['jump', 'to', 'the', 'rope'], ['rope']), (['jump', 'to', 'the', 'rope'], ['rope']), (['jump', 'to', 'the', 'rope'], ['rope']), (['use', 'the', 'key'], ['key']), (['use', 'the', 'key'], ['key'])]\n"
     ]
    }
   ],
   "source": [
    "print (instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent,label in instructions:\n",
    "\tfor word in sent:\n",
    "\t\tif word not in word_to_ix:\n",
    "\t\t\tword_to_ix[word] = len(word_to_ix)\n",
    "\tfor lab in label:\n",
    "\t\tif lab not in label_to_ix:\n",
    "\t\t\tlabel_to_ix[lab] = len(label_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'climb': 0, 'down': 1, 'the': 2, 'ladder': 3, 'up': 4, 'get': 5, 'key': 6, 'sword': 7, 'torch': 8, 'go': 9, 'between': 10, 'lasers': 11, 'to': 12, 'bottom': 13, 'of': 14, 'room': 15, 'center': 16, 'left': 17, 'side': 18, 'right': 19, 'top': 20, 'jump': 21, 'rope': 22, 'use': 23}\n",
      "{'ladder': 0, 'key': 1, 'sword': 2, 'torch': 3, 'laser': 4, 'room': 5, 'rope': 6}\n"
     ]
    }
   ],
   "source": [
    "print(word_to_ix)\n",
    "print(label_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sentence(sent, to_ix):\n",
    "\tsent = sent.lower().strip().split(' ')\n",
    "\tidxs = [to_ix[w] for w in sent]\n",
    "\treturn torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\t\n",
    "\t\tsuper(LSTMClassifier, self).__init__()\n",
    "\n",
    "\t\tself.embeddings = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)\n",
    "\t\tself.lstm = nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM_LSTM)\n",
    "\t\tself.fullyconnected = nn.Linear(HIDDEN_DIM_LSTM, 10)\n",
    "\t\tself.hidden = self.init_hidden()\n",
    "\n",
    "\tdef init_hidden(self):\n",
    "\t\t# the first is the hidden h\n",
    "\t\t# the second is the cell  c\n",
    "\t\treturn (autograd.Variable(torch.zeros(1, 1, HIDDEN_DIM_LSTM)),\n",
    "                autograd.Variable(torch.zeros(1, 1, HIDDEN_DIM_LSTM)))\n",
    "\n",
    "\tdef forward(self, sentence):\n",
    "\n",
    "\t\tembeds = self.embeddings(sentence)\n",
    "\t\tx = embeds.view(len(sentence), 1, -1)\n",
    "\t\tlstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "\t\t#print (lstm_out)\n",
    "\t\ty  = self.fullyconnected(lstm_out[-1])\n",
    "\t\t# log_probs = F.log_softmax(y)\n",
    "\t\t#print (y)\n",
    "\t\treturn y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetClassifier(nn.Module):\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\t\n",
    "\t\tsuper(ConvNetClassifier, self).__init__()\n",
    "\n",
    "\t\tself.layer1 = nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(6, 32, kernel_size = 5, stride = 1, padding = 2),\n",
    "\t\t\t\tnn.ReLU(),\n",
    "\t\t\t\tnn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "\t\t\t)\n",
    "\n",
    "\t\tself.layer2 = nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(32, 32, kernel_size = 5, stride = 1, padding = 2),\n",
    "\t\t\t\tnn.ReLU(),\n",
    "\t\t\t\tnn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "\t\t\t)\n",
    "\n",
    "\t\tself.layer3 = nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(32, 64, kernel_size = 4, stride = 1, padding = 2),\n",
    "\t\t\t\tnn.ReLU(),\n",
    "\t\t\t\tnn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "\t\t\t)\n",
    "\n",
    "\t\tself.layer4 = nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(64, 64, kernel_size = 3, stride = 1, padding = 1)\n",
    "\t\t\t)\n",
    "\n",
    "\t\tself.layer5 = nn.Linear(26*20*64 , 10)\n",
    "\n",
    "\t\tself.layer6 = nn.PReLU()\n",
    "\n",
    "\t\tself.layer7 = nn.Linear(10, 10)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\n",
    "\t\tx = np.swapaxes(x,0,2)\n",
    "\t\tx = np.swapaxes(x,1,2)\n",
    "\n",
    "\t\tx = autograd.Variable(torch.from_numpy(x).unsqueeze(0).float())\n",
    "\n",
    "\t\tout = self.layer1(x)\n",
    "\t\tout = self.layer2(out)\n",
    "\t\tout = self.layer3(out)\n",
    "\t\tout = self.layer4(out)\n",
    "\t\tout = out.view(out.size(0), -1)\n",
    "\t\tout = self.layer5(out)\n",
    "\t\tout = self.layer6(out)\n",
    "\t\tout = self.layer7(out)\n",
    "\t\t#print (out)\n",
    "\t\t\n",
    "\t\treturn out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 20\n",
    "HIDDEN_DIM_LSTM = 10\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "LABEL_SIZE = len(label_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = LSTMClassifier()\n",
    "image_model = ConvNetClassifier()\n",
    "loss_function = nn.CosineEmbeddingLoss()\n",
    "optimizer1 = optim.SGD(text_model.parameters(), lr = 0.001)\n",
    "optimizer2 = optim.SGD(image_model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "\n",
    "\twith open('dataset/dataset_true.pickle','rb') as f:\n",
    "\n",
    "\t\tdataset = pickle.load(f)\n",
    "\n",
    "\twith open('dataset/dataset_false.pickle','rb') as g:\n",
    "\t\tdataset_false = pickle.load(g)\n",
    "\n",
    "\tfor epoch in range(100):\n",
    "\n",
    "\t\tt1 = time.time()\n",
    "\n",
    "\t\ttotal_loss = 0.0\n",
    "\n",
    "\t\tfor (frame1,frame2), sentence in dataset[:300]:\n",
    "\n",
    "\t\t\ttext_model.hidden = text_model.init_hidden()\n",
    "\t\t\t\n",
    "\t\t\ttext_model.zero_grad()\n",
    "\t\t\timage_model.zero_grad()\n",
    "\n",
    "\t\t\tenc_sentence = prepare_sentence(sentence, word_to_ix)\n",
    "\n",
    "\t\t\ttext_embed = text_model(enc_sentence)\n",
    "\n",
    "\t\t\tstack = np.dstack((frame1,frame2))\n",
    "\n",
    "\t\t\tframe_embed = image_model(stack)\n",
    "\n",
    "\t\t\tloss = loss_function(text_embed, frame_embed,torch.tensor([1]).float())\n",
    "\n",
    "\t\t\ttotal_loss += loss.item()\n",
    "\n",
    "\t\t\tloss.backward()\n",
    "\n",
    "\t\t\ttorch.nn.utils.clip_grad_norm(text_model.parameters(),1)\n",
    "\t\t\ttorch.nn.utils.clip_grad_norm(image_model.parameters(),1)\n",
    "\n",
    "\t\t\toptimizer1.step()\n",
    "\t\t\toptimizer2.step()\n",
    "\n",
    "\t\t\tind = np.random.randint(0,10000,5)\n",
    "\n",
    "\t\t\tfor j in ind:\n",
    "\n",
    "\t\t\t\ttext_model.hidden = text_model.init_hidden()\n",
    "\t\t\t\n",
    "\t\t\t\ttext_model.zero_grad()\n",
    "\t\t\t\timage_model.zero_grad()\n",
    "\n",
    "\t\t\t\tenc_sentence = prepare_sentence(dataset_false[j][1], word_to_ix)\n",
    "\n",
    "\t\t\t\ttext_embed = text_model(enc_sentence)\n",
    "\n",
    "\t\t\t\tstack = np.dstack(dataset_false[j][0])\n",
    "\n",
    "\t\t\t\tframe_embed = image_model(stack)\n",
    "\n",
    "\t\t\t\tloss = loss_function(text_embed, frame_embed,torch.tensor([-1]).float())\n",
    "\n",
    "\t\t\t\ttotal_loss += loss.item()\n",
    "\n",
    "\t\t\t\tloss.backward()\n",
    "\n",
    "\t\t\t\ttorch.nn.utils.clip_grad_norm(text_model.parameters(),1)\n",
    "\t\t\t\ttorch.nn.utils.clip_grad_norm(image_model.parameters(),1)\n",
    "\n",
    "\t\t\t\toptimizer1.step()\n",
    "\t\t\t\toptimizer2.step()\n",
    "\n",
    "\t\tt2 = time.time()\n",
    "\n",
    "\t\tprint(\"epoch %d loss %f time %f\"%(epoch,total_loss,t2-t1))\n",
    "\n",
    "\t\tif (epoch+1) % 20 == 0:\n",
    "\t\t\ttorch.save(text_model, 'models/sentence/text_model_' + str(epoch+1))\n",
    "\t\t\ttorch.save(image_model, 'models/image/image_model_' + str(epoch+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sayak\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "C:\\Users\\Sayak\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "C:\\Users\\Sayak\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:66: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "C:\\Users\\Sayak\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 311.436743 time 247.544576\n",
      "epoch 1 loss 299.015654 time 156.242164\n",
      "epoch 2 loss 289.814876 time 127.960447\n",
      "epoch 3 loss 281.724949 time 137.589285\n",
      "epoch 4 loss 268.464626 time 126.810302\n",
      "epoch 5 loss 251.551462 time 125.800061\n",
      "epoch 6 loss 235.933428 time 125.547859\n",
      "epoch 7 loss 222.313873 time 125.510409\n",
      "epoch 8 loss 208.731844 time 124.812763\n",
      "epoch 9 loss 197.394570 time 125.379255\n",
      "epoch 10 loss 182.958803 time 125.113297\n",
      "epoch 11 loss 173.178114 time 125.250710\n",
      "epoch 12 loss 162.841952 time 125.628282\n",
      "epoch 13 loss 153.646677 time 125.869439\n",
      "epoch 14 loss 146.143791 time 126.454809\n",
      "epoch 15 loss 138.492091 time 125.854172\n",
      "epoch 16 loss 127.874507 time 126.711901\n",
      "epoch 17 loss 112.803808 time 125.118589\n",
      "epoch 18 loss 105.526410 time 125.435634\n",
      "epoch 19 loss 104.247145 time 127.637588\n",
      "epoch 20 loss 91.221749 time 134.050073\n",
      "epoch 21 loss 92.232859 time 127.089964\n",
      "epoch 22 loss 91.187590 time 127.221822\n",
      "epoch 23 loss 88.236347 time 129.442087\n",
      "epoch 24 loss 79.841021 time 127.661238\n",
      "epoch 25 loss 79.411936 time 125.690181\n",
      "epoch 26 loss 81.914360 time 126.727661\n",
      "epoch 27 loss 76.623889 time 126.289454\n",
      "epoch 28 loss 79.240690 time 125.401613\n",
      "epoch 29 loss 75.077796 time 127.154186\n",
      "epoch 30 loss 78.642441 time 125.930400\n",
      "epoch 31 loss 68.080106 time 127.225641\n",
      "epoch 32 loss 75.487943 time 127.445371\n",
      "epoch 33 loss 63.594839 time 127.019011\n",
      "epoch 34 loss 67.574159 time 125.023310\n",
      "epoch 35 loss 59.083083 time 128.731237\n",
      "epoch 36 loss 61.051277 time 127.021590\n",
      "epoch 37 loss 55.006384 time 128.057199\n",
      "epoch 38 loss 52.338612 time 126.018449\n",
      "epoch 39 loss 55.838372 time 126.594126\n",
      "epoch 40 loss 50.118873 time 128.411284\n",
      "epoch 41 loss 54.514151 time 127.109202\n",
      "epoch 42 loss 52.348713 time 127.446512\n",
      "epoch 43 loss 49.559826 time 125.947945\n",
      "epoch 44 loss 49.984639 time 126.566901\n",
      "epoch 45 loss 43.165395 time 127.770874\n",
      "epoch 46 loss 42.885965 time 126.727323\n",
      "epoch 47 loss 39.793548 time 127.550773\n",
      "epoch 48 loss 42.318518 time 128.410351\n",
      "epoch 49 loss 41.581294 time 127.130068\n",
      "epoch 50 loss 41.017621 time 128.031129\n",
      "epoch 51 loss 39.775644 time 127.042305\n",
      "epoch 52 loss 39.523597 time 127.363595\n",
      "epoch 53 loss 42.468545 time 128.417046\n",
      "epoch 54 loss 38.630113 time 127.851128\n",
      "epoch 55 loss 40.027708 time 125.853400\n",
      "epoch 56 loss 39.214483 time 126.438392\n",
      "epoch 57 loss 36.367650 time 127.658320\n",
      "epoch 58 loss 36.200534 time 127.027444\n",
      "epoch 59 loss 32.505219 time 127.531971\n",
      "epoch 60 loss 33.745907 time 127.459908\n",
      "epoch 61 loss 36.267693 time 128.112056\n",
      "epoch 62 loss 33.858478 time 129.139589\n",
      "epoch 63 loss 35.896547 time 127.566537\n",
      "epoch 64 loss 31.288760 time 127.097129\n",
      "epoch 65 loss 34.060799 time 127.597355\n",
      "epoch 66 loss 32.100982 time 127.715595\n",
      "epoch 67 loss 31.190314 time 127.007020\n",
      "epoch 68 loss 26.893537 time 128.234600\n",
      "epoch 69 loss 31.482821 time 127.675031\n",
      "epoch 70 loss 29.245483 time 128.291184\n",
      "epoch 71 loss 30.491833 time 126.144032\n",
      "epoch 72 loss 27.570187 time 128.255879\n",
      "epoch 73 loss 27.733200 time 127.848881\n",
      "epoch 74 loss 26.311902 time 126.500090\n",
      "epoch 75 loss 27.565631 time 127.160277\n",
      "epoch 76 loss 28.729823 time 126.803632\n",
      "epoch 77 loss 27.782336 time 127.775318\n",
      "epoch 78 loss 28.950648 time 127.719747\n",
      "epoch 79 loss 28.387758 time 129.071702\n",
      "epoch 80 loss 26.798180 time 128.208201\n",
      "epoch 81 loss 26.031372 time 127.780815\n",
      "epoch 82 loss 23.418865 time 128.009774\n",
      "epoch 83 loss 22.842442 time 128.018754\n",
      "epoch 84 loss 29.527686 time 127.955555\n",
      "epoch 85 loss 25.791911 time 128.027424\n",
      "epoch 86 loss 23.004034 time 126.938058\n",
      "epoch 87 loss 23.583448 time 127.527086\n",
      "epoch 88 loss 22.800785 time 129.325524\n",
      "epoch 89 loss 23.149484 time 128.150380\n",
      "epoch 90 loss 23.392482 time 129.091045\n",
      "epoch 91 loss 22.524507 time 128.437037\n",
      "epoch 92 loss 22.617679 time 131.000533\n",
      "epoch 93 loss 23.476939 time 129.133137\n",
      "epoch 94 loss 22.076417 time 128.946442\n",
      "epoch 95 loss 20.858464 time 129.438949\n",
      "epoch 96 loss 16.930440 time 129.887261\n",
      "epoch 97 loss 20.980267 time 128.281490\n",
      "epoch 98 loss 20.743749 time 129.357926\n",
      "epoch 99 loss 20.616271 time 128.526278\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_dataset():\n",
    "\n",
    "\t#text_model = torch.load('models/text_model_50')\n",
    "\t#image_model = torch.load('models/image_model_50')\n",
    "\n",
    "\twith open('dataset/dataset_true.pickle','rb') as f:\n",
    "\t\tdataset = pickle.load(f)\n",
    "\n",
    "\tdataset_false = []\n",
    "\n",
    "\tfor i in range(300):\n",
    "\t\tfor j in range(300):\n",
    "\n",
    "\t\t\tif dataset[i][1] != dataset[j][1]:\n",
    "\n",
    "\t\t\t\tdataset_false.append((dataset[i][0],dataset[j][1]))\n",
    "\t\t\t\tdataset_false.append((dataset[j][0],dataset[i][1]))\n",
    "\n",
    "\tprint (len(dataset_false))\n",
    "\n",
    "\twith open('dataset/dataset_false.pickle','wb') as f:\n",
    "\t\tpickle.dump(dataset_false,f)\n",
    "\n",
    "\ttest_dataset_false = []\n",
    "\n",
    "\tfor i in range(301, 347):\n",
    "\t\tfor j in range(301, 347):\n",
    "\n",
    "\t\t\tif dataset[i][1] != dataset[j][1]:\n",
    "\n",
    "\t\t\t\ttest_dataset_false.append((dataset[i][0],dataset[j][1]))\n",
    "\t\t\t\ttest_dataset_false.append((dataset[j][0],dataset[i][1]))\n",
    "\n",
    "\tprint (len(test_dataset_false))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142044\n",
      "3384\n"
     ]
    }
   ],
   "source": [
    "false_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "\n",
    "\ttext_model = torch.load('models/sentence/text_model_60')\n",
    "\timage_model = torch.load('models/image/image_model_60')\n",
    "\n",
    "\t# True labels\n",
    "\n",
    "\twith open('dataset/dataset_true.pickle','rb') as f:\n",
    "\t\ttrue_dataset = pickle.load(f)\n",
    "\n",
    "\titems = np.random.randint(301, 347, 15)\n",
    "\n",
    "\titer = 1\n",
    "\tfor i in items:\n",
    "\t\t(img1, img2), text = true_dataset[i]\n",
    "\t\t#img1 = img1[:,:,::-1]\n",
    "\t\t#img2 = img2[:,:,::-1]\n",
    "\t\t'''\n",
    "\t\tenc_sentence = prepare_sentence(text, word_to_ix)\n",
    "\t\ttext_embed = text_model(enc_sentence)\n",
    "\t\tstack = np.dstack((img1, img2))\n",
    "\t\tframe_embed = image_model(stack)\n",
    "\n",
    "\t\tdp = torch.dot(text_embed[0], frame_embed[0]) / (torch.norm(text_embed[0]) * torch.norm(frame_embed[0]))\n",
    "\t\tprint(dp)\n",
    "\t\t'''\n",
    "\t\t#both = np.hstack((img1, img2))\n",
    "\t\tc1 = cv2.copyMakeBorder(img1,10,10,10,10,cv2.BORDER_CONSTANT,value=[255,255,255])\n",
    "\t\tc2 = cv2.copyMakeBorder(img2,10,10,10,10,cv2.BORDER_CONSTANT,value=[255,255,255])\n",
    "\n",
    "\t\tboth = np.hstack((c1,c2))\n",
    "\t\tprint (text)\n",
    "\t\tcv2.imshow('sample', both)\n",
    "\t\tcv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Climb up the ladder\n",
      "Go to the center of the room\n",
      "Go to the center of the room\n",
      "Jump to the rope\n",
      "Jump to the rope\n",
      "Go to the center of the room\n",
      "Jump to the rope\n",
      "Climb up the ladder\n",
      "Go to the right room\n",
      "Use the key\n",
      "Go to the center of the room\n",
      "Go to the center of the room\n",
      "Climb up the ladder\n",
      "Use the key\n",
      "Go to the center of the room\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

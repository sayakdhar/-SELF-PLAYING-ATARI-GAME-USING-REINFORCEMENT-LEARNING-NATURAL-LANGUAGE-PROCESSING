{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import torch.autograd as autograd\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import reward\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi = 1000\n",
    "action_space = 18\n",
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "\n",
    "\tdef __init__(self, output_size = 18, gamma = 0.95):\n",
    "\n",
    "\t\tsuper(ConvNet, self).__init__()\n",
    "\n",
    "\t\tself.layer1 = nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(3, 32, kernel_size = 5, stride = 1, padding = 2),\n",
    "\t\t\t\tnn.ReLU(),\n",
    "\t\t\t\tnn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "\t\t\t)\n",
    "\n",
    "\t\tself.layer2 = nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(32, 32, kernel_size = 5, stride = 1, padding = 2),\n",
    "\t\t\t\tnn.ReLU(),\n",
    "\t\t\t\tnn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "\t\t\t)\n",
    "\n",
    "\t\tself.layer3 = nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(32, 64, kernel_size = 4, stride = 1, padding = 2),\n",
    "\t\t\t\tnn.ReLU(),\n",
    "\t\t\t\tnn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "\t\t\t)\n",
    "\n",
    "\t\tself.layer4 = nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(64, 64, kernel_size = 3, stride = 1, padding = 1)\n",
    "\t\t\t)\n",
    "\n",
    "\t\tself.layer5 = nn.Linear(26*20*64 , output_size)\n",
    "\n",
    "\t\tself.layer6 = nn.PReLU()\n",
    "\n",
    "\t\tself.layer7 = nn.Softmax(dim = 0) # 1-10 Policy Prediction 11 - Value\n",
    "\n",
    "\t\tself.saved_log_probs = []\n",
    "\t\tself.rewards = []\n",
    "\t\tself.gamma = gamma\n",
    "\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\n",
    "\t\tout = self.layer1(x)\n",
    "\t\tout = self.layer2(out)\n",
    "\t\tout = self.layer3(out)\n",
    "\t\tout = self.layer4(out)\n",
    "\t\tout = out.view(out.size(0), -1)\n",
    "\t\tout = self.layer5(out)\n",
    "\t\tout = self.layer6(out)\n",
    "\t\tout = self.layer7(out)\n",
    "\t\t\n",
    "\t\treturn out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(action_space, gamma)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
    "eps = np.finfo(np.float64).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "\n",
    "\tstate = np.array([state[:,:,0],state[:,:,1],state[:,:,2]])\n",
    "\tstate = torch.from_numpy(state).float().unsqueeze(0)\n",
    "\n",
    "\tprobs = model.forward(state)\n",
    "\tm = Categorical(probs)\n",
    "\taction = m.sample()\n",
    "\tmodel.saved_log_probs.append(m.log_prob(action))\n",
    "\treturn action.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = np.zeros(epi)\n",
    "inv_disc = np.zeros(epi)\n",
    "disc[0] = 1.0\n",
    "inv_disc[0] = 1.0\n",
    "for i in range(1,epi):\n",
    "\tdisc[i] = gamma * disc[i-1]\n",
    "\tinv_disc[i] = inv_disc[i-1] / gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode():\n",
    "\n",
    "\tl = len(model.rewards) # Length of Episode\n",
    "\n",
    "\tif l == 0:\n",
    "\t\treturn\n",
    "\n",
    "\tmodel_rewards = np.array(model.rewards)\n",
    "\t_rewards = disc[:l] * model_rewards\n",
    "\tcumulative_rewards = np.cumsum(_rewards[::-1])[::-1]\n",
    "\trewards = cumulative_rewards * inv_disc[:l]\n",
    "\n",
    "\tprint (rewards)\n",
    "\n",
    "\trewards = torch.from_numpy(rewards).float()\n",
    "\trewards = (rewards - rewards.mean()) / (rewards.std() + eps)\n",
    "\n",
    "\tpolicy_loss = []\n",
    "\n",
    "\tfor log_prob, reward in zip(model.saved_log_probs, rewards):\n",
    "\t\tpolicy_loss.append(- log_prob * reward)\n",
    "\n",
    "\toptimizer.zero_grad()\n",
    "\tpolicy_loss = torch.cat(policy_loss).sum()\n",
    "\tpolicy_loss.backward()\n",
    "\toptimizer.step()\n",
    "\n",
    "\tdel model.rewards[:]\n",
    "\tdel model.saved_log_probs[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\t\n",
    "\t\tsuper(LSTMClassifier, self).__init__()\n",
    "\n",
    "\t\tself.embeddings = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)\n",
    "\t\tself.lstm = nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM_LSTM)\n",
    "\t\tself.fullyconnected = nn.Linear(HIDDEN_DIM_LSTM, 10)\n",
    "\t\tself.hidden = self.init_hidden()\n",
    "\n",
    "\tdef init_hidden(self):\n",
    "\t\t# the first is the hidden h\n",
    "\t\t# the second is the cell  c\n",
    "\t\treturn (autograd.Variable(torch.zeros(1, 1, HIDDEN_DIM_LSTM)),\n",
    "                autograd.Variable(torch.zeros(1, 1, HIDDEN_DIM_LSTM)))\n",
    "\n",
    "\tdef forward(self, sentence):\n",
    "\n",
    "\t\tembeds = self.embeddings(sentence)\n",
    "\t\tx = embeds.view(len(sentence), 1, -1)\n",
    "\t\tlstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "\t\t#print (lstm_out)\n",
    "\t\ty  = self.fullyconnected(lstm_out[-1])\n",
    "\t\t# log_probs = F.log_softmax(y)\n",
    "\t\t#print (y)\n",
    "\t\treturn y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetClassifier(nn.Module):\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\t\n",
    "\t\tsuper(ConvNetClassifier, self).__init__()\n",
    "\n",
    "\t\tself.layer1 = nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(6, 32, kernel_size = 5, stride = 1, padding = 2),\n",
    "\t\t\t\tnn.ReLU(),\n",
    "\t\t\t\tnn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "\t\t\t)\n",
    "\n",
    "\t\tself.layer2 = nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(32, 32, kernel_size = 5, stride = 1, padding = 2),\n",
    "\t\t\t\tnn.ReLU(),\n",
    "\t\t\t\tnn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "\t\t\t)\n",
    "\n",
    "\t\tself.layer3 = nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(32, 64, kernel_size = 4, stride = 1, padding = 2),\n",
    "\t\t\t\tnn.ReLU(),\n",
    "\t\t\t\tnn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "\t\t\t)\n",
    "\n",
    "\t\tself.layer4 = nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(64, 64, kernel_size = 3, stride = 1, padding = 1)\n",
    "\t\t\t)\n",
    "\n",
    "\t\tself.layer5 = nn.Linear(26*20*64 , 10)\n",
    "\n",
    "\t\tself.layer6 = nn.PReLU()\n",
    "\n",
    "\t\tself.layer7 = nn.Linear(10, 10)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\n",
    "\t\tx = np.swapaxes(x,0,2)\n",
    "\t\tx = np.swapaxes(x,1,2)\n",
    "\n",
    "\t\tx = autograd.Variable(torch.from_numpy(x).unsqueeze(0).float())\n",
    "\n",
    "\t\tout = self.layer1(x)\n",
    "\t\tout = self.layer2(out)\n",
    "\t\tout = self.layer3(out)\n",
    "\t\tout = self.layer4(out)\n",
    "\t\tout = out.view(out.size(0), -1)\n",
    "\t\tout = self.layer5(out)\n",
    "\t\tout = self.layer6(out)\n",
    "\t\tout = self.layer7(out)\n",
    "\t\t#print (out)\n",
    "\t\t\n",
    "\t\treturn out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {}\n",
    "label_to_ix = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('instructions.txt','r') as f:\n",
    "\n",
    "\tf = f.readlines()\n",
    "\t\n",
    "\tfor line in f:\n",
    "\n",
    "\t\tline = line.strip().split(',')\n",
    "\t\tlabel = [line[1]]\n",
    "\t\tsentence = list(map(lambda x : x.lower(),line[0].strip().split(' ')))\n",
    "\t\tinstructions.append((sentence,label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent,label in instructions:\n",
    "\tfor word in sent:\n",
    "\t\tif word not in word_to_ix:\n",
    "\t\t\tword_to_ix[word] = len(word_to_ix)\n",
    "\tfor lab in label:\n",
    "\t\tif lab not in label_to_ix:\n",
    "\t\t\tlabel_to_ix[lab] = len(label_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_to_ix)\n",
    "print(label_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sentence(sent, to_ix):\n",
    "\tsent = sent.lower().strip().split(' ')\n",
    "\tidxs = [to_ix[w] for w in sent]\n",
    "\treturn torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\t\n",
    "\tenv = gym.make('MontezumaRevenge-v0')\n",
    "\n",
    "\ttext_model = torch.load('models/sentence/text_model_40')\n",
    "\timage_model = torch.load('models/image/image_model_40')\n",
    "\tmodel = torch.load('models/policy/model_100')\n",
    "\n",
    "\twith open('dataset/dataset_true.pickle','rb') as f:\n",
    "\t\tdataset = pickle.load(f)\n",
    "\n",
    "\twith open('first_room.txt','r') as f:\n",
    "\t\tinstructions = f.readlines()\n",
    "\n",
    "\tinstructions = [i.strip() for i in instructions]\n",
    "\n",
    "\tprint (instructions)\n",
    "\n",
    "\tfor episode in range(1000):\n",
    "\n",
    "\t\tt1 = time.time()\n",
    "\n",
    "\t\tstate = env.reset()\n",
    "\t\tepisode_reward = 0\n",
    "\n",
    "\t\tinfo = {'ale.lives':6}\n",
    "\n",
    "\t\t#if episode % 10 == 0 and episode != 0:\n",
    "\n",
    "\t\t#\ttorch.save(model, 'saved/model_' + str(episode))\n",
    "\t\t#\ttorch.save(model.state_dict(), 'saved/parameter_' + str(episode))\n",
    "\n",
    "\t\tfor t in range(epi):\n",
    "\t\t\t\n",
    "\t\t\taction = select_action(state)\n",
    "\n",
    "\t\t\tstate_new, reward, done, info_new = env.step(action)\n",
    "\t\t\t\n",
    "\t\t\tstack = np.dstack((state,state_new))\n",
    "\n",
    "\t\t\tframe_embed = image_model(stack)\n",
    "\n",
    "\t\t\tenc_sentence = prepare_sentence(instructions[0], word_to_ix)\n",
    "\n",
    "\t\t\ttext_embed = text_model(enc_sentence)\n",
    "\n",
    "\t\t\treward = torch.dot(text_embed[0], frame_embed[0])\n",
    "\t\t\t\n",
    "\t\t\tr = reward.item()\n",
    "\n",
    "\t\t\t#print (r)\n",
    "\n",
    "\t\t\tif r < 0.9: r = 0\n",
    "\n",
    "\t\t\tmodel.rewards.append(r)\n",
    "\n",
    "\t\t\tenv.render()\n",
    "\n",
    "\t\t\tinfo = info_new\n",
    "\t\t\tstate = state_new\n",
    "\t\t\t#if episode % 20 == 0:\n",
    "\t\t\t#\tenv.render()\n",
    "\n",
    "\t\t\t#if (t+1) % 40 == 0:\n",
    "\t\t\t#\tfinish_episode()\n",
    "\t\t\n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\tt2 = time.time()\n",
    "\n",
    "\t\tprint('Episode %d\\t Episode Reward: %f\\t Time: %f'%(episode, episode_reward, t2 - t1))\n",
    "\n",
    "\tenv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
